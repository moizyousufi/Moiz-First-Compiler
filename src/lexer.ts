/**
 * lexer.ts
 * Adapted and edited from Peter-Michael Osera
 * 
 * Takes a string input and lexes it into tokens for the parser to read
 */

/***** Lexer Datatypes ********************************************************/

import * as L from './lang'

export type LexerState = { index: number }



export type TInt = { tag: "int"; value: number }
const tint = (value: number): Tok => ({ tag: "int", value })

export type TFLPO = { tag: "flpo"; value: number }
const tflpo = (value: number): Tok => ({ tag: "flpo", value })



export type TLParen = { tag: "(" }
const tlparen: Tok = { tag: "(" }

export type TRParen = { tag: ")" }
const trparen: Tok = { tag: ")" }


// object related
export type TLBrack = { tag: "{" }
const tlbrack: Tok = { tag: "{" }

export type TRBrack = { tag: "}" }
const trbrack: Tok = { tag: "}" }

export type TColon = { tag: ":" }
const tcolon: Tok = { tag: ":" }

export type TComma = { tag: "," }
const tcomma: Tok = { tag: ',' }

export type TDot = { tag: "." }
const tdot: Tok = { tag: '.' }


// variable
export type TDefine = { tag: "Define" }
const tdefine: Tok = { tag: 'Define' }

export type TCall = { tag: "Call" }
const tcall: Tok = { tag: 'Call' }


export type TAdd = { tag: "add" }
const tadd: Tok = { tag: "add" }



export type TNull = { tag: "null" }
const tnull: Tok = { tag: "null" }

export type Semicolon = { tag: "semicolon" }
const tSemicolon: Tok = { tag: 'semicolon'}

export type TIdent = { tag: "ident"; value: string }
const tident = (value: string): Tok => ({ tag: "ident", value })

/** The type of tokens generated by the lexer. */
export type Tok = TInt | TFLPO | TLParen | TRParen | TLBrack | TRBrack | TAdd | TNull | Semicolon | TIdent | TColon | TComma | TDot | TDefine | TCall

/***** Lexer Function *********************************************************/

/**
 * @returns a pretty version of the input token `tok`.
 */
export function prettyTok(tok: Tok): string 
{
    switch (tok.tag) 
    {
        case "int":
            return tok.value.toString()
        case "flpo":
            return tok.value.toString()
        case "(":
            return "("
        case ")":
            return ")"
        case "{":
            return "{"
        case "}":
            return "}"
        case '.':
            return '.'
        case 'Define':
            return 'Define'
        case 'Call':
            return 'Call'
        case "add":
            return '+'
        case 'semicolon':
            return ';'
        case "null":
            return "null"
        case ':':
            return ':'
        case ',':
            return ','
        case 'ident':
            return tok.value
    }
}

function tokenize(state: LexerState, src: string): Tok[] 
{
    const ret: Tok[] = []
    while (state.index < src.length) 
    {
        // Skip over whitespace
        while (/\s/.test(src[state.index])) 
        {
            ++state.index
        }

        const leader = src[state.index]
        if (leader === "(") 
        {
            ++state.index
            ret.push(tlparen)
        } 
        else if (leader === ")") 
        {
            ++state.index
            ret.push(trparen)
        } 
        else if (leader === "{") 
        {
            ++state.index
            ret.push(tlbrack)
        } 
        else if (leader === "}") 
        {
            ++state.index
            ret.push(trbrack)
        } 
        else if (leader === ":") 
        {
            ++state.index
            ret.push(tcolon)
        } 
        else if (leader === ",") 
        {
            ++state.index
            ret.push(tcomma)
        } 
        else if (leader === ';') 
        {
            ++state.index
            ret.push(tSemicolon)
        }
        else if (leader === 'Define') 
        {
            ++state.index
            ret.push(tdefine)
        }
        else if (leader === 'Call') 
        {
            ++state.index
            ret.push(tcall)
        }
        else if (/\d/.test(leader)) 
        {
            let digits = leader
            while (/\d/.test(src[++state.index])) 
            {
                digits += src[state.index]
            }
            if(src[state.index] === '.')
            {
                digits += src[state.index]
                while (/\d/.test(src[++state.index])) 
                {
                    digits += src[state.index]
                }
            }
            // if digits matches with a floating point number
            // and NOT an integer
            if(digits.match(/^-?\d*\.\d+(?!\d)$/))
                ret.push(tflpo(parseFloat(digits)))
            // if digits can be tokenized as an integer
            else
                ret.push(tint(parseInt(digits)))
        }
        else
        {
            // N.B., identifiers are tokens that do not start with a digit.
            let chk = leader
            let cur = src[++state.index]
            while ( state.index < src.length &&
                    /\S/.test(cur) &&
                    cur !== "(" &&
                    cur !== ")") 
            {
                chk += cur
                cur = src[++state.index]
            }
            if (chk === "null") 
            {
                ret.push(tnull)
            } 
            else 
            {
                ret.push(tident(chk))
            }
        }
    }
    return ret
}

/**
 * @returns an array of tokens created by lexing `src`.
 */
export function lex(src: string): Tok[] 
{
    return tokenize({ index: 0 }, src)
}
